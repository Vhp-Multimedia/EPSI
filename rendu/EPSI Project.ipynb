{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPSI M2 ISD \n",
    "# Handy Pedro VALERY & Charles Dehlinger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from IPython.core.display import display, HTML\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "\n",
    "#display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import re\n",
    "import random\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting global variables\n",
    "rgx = re.compile(\"([\\w][\\w']*\\w)\")\n",
    "text = open(\"train.txt\").readlines()\n",
    "textLen = len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning the text into 5 portions to used with crossed validations\n",
    "# Text : Corpus\n",
    "# K    : Potions\n",
    "\n",
    "def textToPartions(text,k):\n",
    "    portions=[]\n",
    "    increments = round(textLen/k)\n",
    "    for i in range(0,textLen,increments):\n",
    "        portions.append((i,i+increments-1))\n",
    "        \n",
    "    return portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreates the original text from a list of portions tuples\n",
    "# Text : Corpus\n",
    "# Portions : List of Portions index to retrieve in the text. \n",
    "\n",
    "def portionsToSubtext(textArray, portions):\n",
    "    resText=[]\n",
    "    for i,j in portions:\n",
    "        for l in textArray[i:j+1]:\n",
    "            resText.append(l)\n",
    "            \n",
    "    return resText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Cross Validation Portions (Train, Test, Validation)\n",
    "\n",
    "def createShuffledSubText(originalText):\n",
    "     \n",
    "    partitions = textToPartions(originalText,5)\n",
    "    result = { 'Train-Test': [], 'validation': partitions.pop()}\n",
    "    \n",
    "    for i in range(len(partitions)):\n",
    "        \n",
    "        curr = {'train' : [partitions[i],partitions[(i+1)%4],partitions[(i+2)%4]], \n",
    "                'test'  : [partitions[(i+3)%4]],\n",
    "                'language':{}\n",
    "                ,'words'  :{}\n",
    "                , 'score':0, 'failed':[]}\n",
    "        \n",
    "        result['Train-Test'].append(curr)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_text):\n",
    "    \n",
    "    # list of words\n",
    "    # dict of words\n",
    "    # total of phrases\n",
    "    \n",
    "    tokens_dict = {}\n",
    "    tokens_list = []\n",
    "    phrases_cnt = 0;\n",
    "    \n",
    "    for s in input_text.lower().split(\". \"):\n",
    "        phrases_cnt +=1\n",
    "        \n",
    "        for w in rgx.findall(s):\n",
    "            tokens_list.append(w)\n",
    "            tokens_dict[w] = ((tokens_dict[w][0])+1,'Pure') if w in tokens_dict else (1,'Pure')\n",
    "            \n",
    "    return tokens_list, tokens_dict, phrases_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCounter(row,language,TTP):\n",
    "    \n",
    "    # Input the corpus as an array of inputs paragraphes, a target language, returns:\n",
    "    # list of (length of words in a dictionary, [distinct words count, sentences count], the dictionary of words)\n",
    "    \n",
    "    tk = tokenize(row)\n",
    "    for w in tk[1]:\n",
    "        \n",
    "        # Processing the lang dictionary\n",
    "        #dict[w]= ((dict[w][0])+1,'Pure') if w in dict else (1,'Pure')\n",
    "        \n",
    "        w_occurence = tk[1][w][0]  \n",
    "        #print(w_occurence)\n",
    "        #Processing the global dictionary\n",
    "            \n",
    "        if w not in TTP['words']: \n",
    "                               \n",
    "            TTP['words'][w]= {'occurences':w_occurence,'language':{language: w_occurence }}\n",
    "            \n",
    "        elif w in TTP['words']:\n",
    "            \n",
    "            TTP['words'][w]['occurences']+= w_occurence\n",
    "            \n",
    "            if language not in TTP['words'][w]['language']:\n",
    "                \n",
    "                TTP['words'][w]['language'][language] = w_occurence\n",
    "            else:\n",
    "                #print(TTP['words'][w][language])\n",
    "                TTP['words'][w]['language'][language]+= w_occurence\n",
    "            \n",
    "    return tk[0], tk[1], tk[2], TTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDict(d1, d2):\n",
    "    for w in d2:\n",
    "        if w not in d1:\n",
    "            d1[w]=d2[w]\n",
    "        else: \n",
    "            d1[w]=((d1[w][0]+d2[w][0]),d1[w][1])\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(a,b):\n",
    "    \n",
    "    for w in a:\n",
    "        \"\"\"\n",
    "        if type(a[w])=='List' and a[w][1]=='Dirty':\n",
    "            continue;\n",
    "        if type(a[w])=='int' and w in b:\n",
    "           \n",
    "            a[w]=[a[w],'Dirty']         \n",
    "        else :\n",
    "            a[w]=[a[w],'Pure']\n",
    "        \"\"\"\n",
    "        if a[w][1]=='Dirty':\n",
    "            continue;\n",
    "        elif w in b: \n",
    "            a[w]=(a[w][0],'Dirty')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter(l1, l2):\n",
    "    dict = {}\n",
    "    for e in l2:\n",
    "        if e in l1:\n",
    "            dict[e]=0\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Global variables containing (Train, Test, Validation) for a total of 4 Portions of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = createShuffledSubText(text)\n",
    "Train_Test = Data['Train-Test']\n",
    "Valid = Data['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering statistics for each portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing all text inputs in all partitions\n",
    "for TTP in Train_Test:\n",
    "    \n",
    "    TTP_Train = portionsToSubtext(text,TTP['train'])\n",
    "    \n",
    "    for i in range(len(TTP_Train)):\n",
    "        \n",
    "        #Processing language dictionnary of TTP partition\n",
    "        lang = TTP_Train[i][1:4]\n",
    "        wc = wordCounter(TTP_Train[i][5:-1],lang,TTP)\n",
    "        TTP=wc[3]#???\n",
    "       \n",
    "        if lang in TTP['language']:\n",
    "\n",
    "            TTP['language'][lang]['Dict'     ] = addDict(TTP['language'][lang]['Dict'],wc[1])\n",
    "            TTP['language'][lang]['Words'    ]+= len(wc[0])      #List of all words in row\n",
    "            TTP['language'][lang]['Phrases'  ]+= wc[2]           #Total of all sentences in row\n",
    "            \n",
    "        else :\n",
    "\n",
    "            TTP['language'][lang] = {'Dict':wc[1],'Words':len(wc[0]),'Phrases':wc[2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Setting up averages on the previous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing all text inputs in all partitions\n",
    "for TTP in Train_Test:\n",
    "    for l in TTP['language']:\n",
    "        w_count    = TTP['language'][l]['Words']\n",
    "        p_count    = TTP['language'][l]['Phrases']\n",
    "        d_distinct = len(TTP['language'][l]['Dict'])\n",
    "        TTP['language'][l]['Distinct']= d_distinct\n",
    "        \n",
    "        TTP['language'][l]['avg_WP']= round(w_count/p_count,2)\n",
    "        TTP['language'][l]['avg_DW']= round(d_distinct/w_count,2)\n",
    "        TTP['language'][l]['avg_DP']= round(d_distinct/p_count,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview of some statistical date for set of languages in each Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition [(0, 1979), (1980, 3959), (3960, 5939)] \n",
      "\n",
      "GER Total Words: 168567 Distinct Words: 8258 Total Phrases: 8732 AVG-WP: 19.3 AVG WD: 0.95 Ratio DW: 0.05\n",
      "TUR Total Words: 166726 Distinct Words: 8506 Total Phrases: 9488 AVG-WP: 17.57 AVG WD: 0.9 Ratio DW: 0.05\n",
      "CHI Total Words: 166209 Distinct Words: 9072 Total Phrases: 9396 AVG-WP: 17.69 AVG WD: 0.97 Ratio DW: 0.05\n",
      "TEL Total Words: 175638 Distinct Words: 9943 Total Phrases: 8957 AVG-WP: 19.61 AVG WD: 1.11 Ratio DW: 0.06\n",
      "ARA Total Words: 151002 Distinct Words: 10253 Total Phrases: 6685 AVG-WP: 22.59 AVG WD: 1.53 Ratio DW: 0.07\n",
      "SPA Total Words: 167930 Distinct Words: 9224 Total Phrases: 7208 AVG-WP: 23.3 AVG WD: 1.28 Ratio DW: 0.05\n",
      "HIN Total Words: 183841 Distinct Words: 10634 Total Phrases: 9288 AVG-WP: 19.79 AVG WD: 1.14 Ratio DW: 0.06\n",
      "JPN Total Words: 150055 Distinct Words: 8017 Total Phrases: 9646 AVG-WP: 15.56 AVG WD: 0.83 Ratio DW: 0.05\n",
      "KOR Total Words: 161829 Distinct Words: 8680 Total Phrases: 10145 AVG-WP: 15.95 AVG WD: 0.86 Ratio DW: 0.05\n",
      "FRE Total Words: 169484 Distinct Words: 8874 Total Phrases: 8567 AVG-WP: 19.78 AVG WD: 1.04 Ratio DW: 0.05\n",
      "ITA Total Words: 154222 Distinct Words: 8233 Total Phrases: 6722 AVG-WP: 22.94 AVG WD: 1.22 Ratio DW: 0.05\n",
      "\n",
      "\n",
      "Partition [(1980, 3959), (3960, 5939), (5940, 7919)] \n",
      "\n",
      "TUR Total Words: 164335 Distinct Words: 8475 Total Phrases: 9257 AVG-WP: 17.75 AVG WD: 0.92 Ratio DW: 0.05\n",
      "CHI Total Words: 171518 Distinct Words: 9276 Total Phrases: 9599 AVG-WP: 17.87 AVG WD: 0.97 Ratio DW: 0.05\n",
      "KOR Total Words: 159867 Distinct Words: 8621 Total Phrases: 10047 AVG-WP: 15.91 AVG WD: 0.86 Ratio DW: 0.05\n",
      "GER Total Words: 171418 Distinct Words: 8332 Total Phrases: 8956 AVG-WP: 19.14 AVG WD: 0.93 Ratio DW: 0.05\n",
      "ARA Total Words: 153123 Distinct Words: 10306 Total Phrases: 6759 AVG-WP: 22.65 AVG WD: 1.52 Ratio DW: 0.07\n",
      "HIN Total Words: 188673 Distinct Words: 10895 Total Phrases: 9627 AVG-WP: 19.6 AVG WD: 1.13 Ratio DW: 0.06\n",
      "ITA Total Words: 155195 Distinct Words: 8226 Total Phrases: 6799 AVG-WP: 22.83 AVG WD: 1.21 Ratio DW: 0.05\n",
      "FRE Total Words: 163451 Distinct Words: 8655 Total Phrases: 8249 AVG-WP: 19.81 AVG WD: 1.05 Ratio DW: 0.05\n",
      "TEL Total Words: 178553 Distinct Words: 10255 Total Phrases: 9068 AVG-WP: 19.69 AVG WD: 1.13 Ratio DW: 0.06\n",
      "SPA Total Words: 162057 Distinct Words: 8944 Total Phrases: 6890 AVG-WP: 23.52 AVG WD: 1.3 Ratio DW: 0.05\n",
      "JPN Total Words: 146595 Distinct Words: 7947 Total Phrases: 9366 AVG-WP: 15.65 AVG WD: 0.85 Ratio DW: 0.05\n",
      "\n",
      "\n",
      "Partition [(3960, 5939), (5940, 7919), (0, 1979)] \n",
      "\n",
      "TUR Total Words: 167114 Distinct Words: 8629 Total Phrases: 9300 AVG-WP: 17.97 AVG WD: 0.93 Ratio DW: 0.05\n",
      "JPN Total Words: 144991 Distinct Words: 8023 Total Phrases: 9340 AVG-WP: 15.52 AVG WD: 0.86 Ratio DW: 0.05\n",
      "KOR Total Words: 160963 Distinct Words: 8627 Total Phrases: 10126 AVG-WP: 15.9 AVG WD: 0.85 Ratio DW: 0.05\n",
      "ARA Total Words: 153523 Distinct Words: 10190 Total Phrases: 6768 AVG-WP: 22.68 AVG WD: 1.51 Ratio DW: 0.07\n",
      "CHI Total Words: 169005 Distinct Words: 9281 Total Phrases: 9390 AVG-WP: 18.0 AVG WD: 0.99 Ratio DW: 0.05\n",
      "FRE Total Words: 169220 Distinct Words: 8965 Total Phrases: 8497 AVG-WP: 19.92 AVG WD: 1.06 Ratio DW: 0.05\n",
      "TEL Total Words: 172553 Distinct Words: 9978 Total Phrases: 8764 AVG-WP: 19.69 AVG WD: 1.14 Ratio DW: 0.06\n",
      "HIN Total Words: 191099 Distinct Words: 10958 Total Phrases: 9614 AVG-WP: 19.88 AVG WD: 1.14 Ratio DW: 0.06\n",
      "GER Total Words: 173140 Distinct Words: 8475 Total Phrases: 9074 AVG-WP: 19.08 AVG WD: 0.93 Ratio DW: 0.05\n",
      "SPA Total Words: 170174 Distinct Words: 9117 Total Phrases: 7277 AVG-WP: 23.39 AVG WD: 1.25 Ratio DW: 0.05\n",
      "ITA Total Words: 149144 Distinct Words: 8052 Total Phrases: 6582 AVG-WP: 22.66 AVG WD: 1.22 Ratio DW: 0.05\n",
      "\n",
      "\n",
      "Partition [(5940, 7919), (0, 1979), (1980, 3959)] \n",
      "\n",
      "CHI Total Words: 176011 Distinct Words: 9524 Total Phrases: 9952 AVG-WP: 17.69 AVG WD: 0.96 Ratio DW: 0.05\n",
      "ITA Total Words: 157180 Distinct Words: 8247 Total Phrases: 6930 AVG-WP: 22.68 AVG WD: 1.19 Ratio DW: 0.05\n",
      "SPA Total Words: 168764 Distinct Words: 9222 Total Phrases: 7203 AVG-WP: 23.43 AVG WD: 1.28 Ratio DW: 0.05\n",
      "KOR Total Words: 157235 Distinct Words: 8534 Total Phrases: 9762 AVG-WP: 16.11 AVG WD: 0.87 Ratio DW: 0.05\n",
      "JPN Total Words: 146386 Distinct Words: 7858 Total Phrases: 9421 AVG-WP: 15.54 AVG WD: 0.83 Ratio DW: 0.05\n",
      "TEL Total Words: 170339 Distinct Words: 9896 Total Phrases: 8749 AVG-WP: 19.47 AVG WD: 1.13 Ratio DW: 0.06\n",
      "GER Total Words: 171058 Distinct Words: 8321 Total Phrases: 8899 AVG-WP: 19.22 AVG WD: 0.94 Ratio DW: 0.05\n",
      "ARA Total Words: 148781 Distinct Words: 9991 Total Phrases: 6506 AVG-WP: 22.87 AVG WD: 1.54 Ratio DW: 0.07\n",
      "FRE Total Words: 168351 Distinct Words: 8854 Total Phrases: 8464 AVG-WP: 19.89 AVG WD: 1.05 Ratio DW: 0.05\n",
      "TUR Total Words: 158114 Distinct Words: 8296 Total Phrases: 8972 AVG-WP: 17.62 AVG WD: 0.92 Ratio DW: 0.05\n",
      "HIN Total Words: 184671 Distinct Words: 10747 Total Phrases: 9403 AVG-WP: 19.64 AVG WD: 1.14 Ratio DW: 0.06\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Enriching TTPs with mean and orther stats\n",
    "for TT in Train_Test:\n",
    "    #Taille Moyenne des phrase en mots\n",
    "    print(\"Partition\", TT['train'],'\\n')\n",
    "    for l in TT['language']:\n",
    "        \n",
    "        print(l,\n",
    "              \"Total Words:\",   TT['language'][l]['Words'],\n",
    "              \"Distinct Words:\",TT['language'][l]['Distinct'],\n",
    "              \"Total Phrases:\", TT['language'][l]['Phrases'],\n",
    "              \"AVG-WP:\",TT['language'][l]['avg_WP'],\n",
    "              \"AVG WD:\",TT['language'][l]['avg_DP'],\n",
    "              \"Ratio DW:\",TTP['language'][l]['avg_DW'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up dirty words for each language of each Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for TT in Train_Test:\n",
    "    \n",
    "    langs = list(TT['language'].keys())\n",
    "    size  = len(langs)\n",
    "\n",
    "    for x in range(size):\n",
    "        for y in range(size):\n",
    "            if x != y:\n",
    "                TT['language'][langs[x]]['D'] = extract(TT['language'][langs[x]]['Dict'],TT['language'][langs[y]]['Dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inegrity test between global dictionnary and language dictionarry for each Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for TT in Train_Test:\n",
    "    #Testing acuracy of both dictionaries \n",
    "    \n",
    "    for word in TT['words'].keys() :\n",
    "        for lang in TT['words'][word]['language'].keys():\n",
    "            if word not in TT['language'][lang]['Dict']:\n",
    "                print(TT['train'],word)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates statistical information about a text input. \n",
    "#  TT     : Portion containing Train, Test, Words, Language ... \n",
    "#  Tokens : results of function Tokens(input_text)\n",
    "#  Debug  : Boolean to preview or not logs from stats when running\n",
    "\n",
    "def stats(TT,tokens,debug):\n",
    "         \n",
    "    langStats = {\n",
    "        \n",
    "        'FRE':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 },\n",
    "        'ARA':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 },\n",
    "        'CHI':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 },\n",
    "        'TUR':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 },\n",
    "        'TEL':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 },\n",
    "        'GER':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 },\n",
    "        'JPN':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 },\n",
    "        'HIN':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 },\n",
    "        'SPA':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 },\n",
    "        'KOR':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 },\n",
    "        'ITA':{ 'pure':0, 'global':0, 'dirty':0, 'unknown':0 }\n",
    "    }\n",
    "     \n",
    "    for each in langStats :\n",
    "        \n",
    "        D = TT['language'][each]['Dict']\n",
    "        D_normalizer = len(tokens)*len(D)\n",
    "        W_normalizer = len(tokens)*len(TT['words'])\n",
    "        \n",
    "        for w in tokens:\n",
    "            \n",
    "            if w in D:\n",
    "                \n",
    "                if D[w][1]=='Pure':\n",
    "                    langStats[each]['pure']+=round(1/D_normalizer,10)\n",
    "                    \n",
    "                elif D[w][1]=='Dirty':\n",
    "                    \n",
    "                    current = D[w][0]\n",
    "                    others  = TT['words'][w]['occurences']-current\n",
    "            \n",
    "                    occ = (current/(others/10))/D_normalizer\n",
    "                    langStats[each]['dirty']+= round(occ,10)\n",
    "                        \n",
    "            elif w in TT['words']:\n",
    "                langStats[each]['global']   +=round(1/W_normalizer, 10)\n",
    "            \n",
    "        if debug :\n",
    "            print(each,langStats[each])\n",
    "        \n",
    "    return langStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction minimisation function\n",
    "# T : Text \n",
    "# TTP : Portion\n",
    "# debug : boolean\n",
    "# returns language with best probabilitie sfor a give input text.\n",
    "\n",
    "def box(t,TTP,debug):\n",
    "     \n",
    "    langStats = stats(TTP,tokenize(t)[0],debug)\n",
    "        \n",
    "    return min(langStats.keys(), key=(lambda k: (1/(langStats[k]['pure']+1))*(langStats[k]['global']+1)/(langStats[k]['dirty']+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the score of the train model on test text samples\n",
    "# returns a score in %\n",
    "\n",
    "def score (TT): \n",
    "    \n",
    "    Test = portionsToSubtext(text,TT['test'])\n",
    "    acc = 0\n",
    "    failed = []\n",
    "    L = len(Test)\n",
    "    for i in range(L):\n",
    "\n",
    "        T=Test[i]\n",
    "        trueLang = T[1:4]\n",
    "\n",
    "        predictedLang = box(T[5:-1],TT,False)\n",
    "        if predictedLang != trueLang:\n",
    "            #print(i,trueLang,predictedLang)\n",
    "            failed.append(i)\n",
    "\n",
    "        if predictedLang == trueLang:\n",
    "            acc+=1/L\n",
    "    return acc,failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation model on all test Portions\n",
    "for TT in Train_Test:\n",
    "    TT['score'],TT['failed']=score(TT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portion 1 score 51.41 %\n",
      "Portion 2 score 50.35 %\n",
      "Portion 3 score 50.76 %\n",
      "Portion 4 score 48.94 %\n"
     ]
    }
   ],
   "source": [
    "# Displaying score for each test Portion\n",
    "i = 1\n",
    "for each in Train_Test:\n",
    "    print(\"Portion\",i, 'score',round(each['score']*100,2),'%')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function use to create feature of random forest model \n",
    "# i   : Not used\n",
    "# T   : Text\n",
    "# TTP : current Portion\n",
    "\n",
    "def createFeature(i,T,TTP):\n",
    "    \n",
    "    \n",
    "    tk = tokenize(T[5:-1])\n",
    "    Stats = stats(TTP,tk[0],False)\n",
    "    \n",
    "    w   = len(tk[0])\n",
    "    d   = len(tk[1])\n",
    "    p   =     tk[2]\n",
    "    \n",
    "    dp  = round(d/p,2)\n",
    "    wp  = round(w/p,2)\n",
    "    dw  = round(d/w,2)\n",
    "    wd  = round(w/d,2)\n",
    "    \n",
    "    features =[w,d,p,dp,wp,dw,wd]\n",
    "      \n",
    "    for s in Stats:\n",
    "        \n",
    "        P = Stats[s]['pure']  +0.1\n",
    "        D = Stats[s]['dirty'] +0.1\n",
    "        G = Stats[s]['global']+0.1\n",
    "        \n",
    "        features.append(P)\n",
    "        features.append(G)     \n",
    "        features.append(G/D)\n",
    "        features.append(D)\n",
    "        features.append((1/P)*(G/D))\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training all Portions using RF classification \n",
    "\n",
    "for TT in Train_Test:\n",
    "    \n",
    "    Train = portionsToSubtext(text,TT['train'])\n",
    "    Test  = portionsToSubtext(text,TT['test' ])\n",
    "    \n",
    "    X_train =[] \n",
    "    y_train =[]\n",
    "\n",
    "    for i in range(len (Train)):\n",
    "        y_train.append(Train[i][1:4])\n",
    "        X_train.append(createFeature(i,Train[i],TT))\n",
    "        \n",
    "    X_test =[] \n",
    "    y_test =[]\n",
    "\n",
    "    for i in range(len(Test)):\n",
    "        y_test.append(Test[i][1:4])\n",
    "        X_test.append(createFeature(i,Test[i],TT))\n",
    "\n",
    "    clf=RandomForestClassifier(n_estimators=1000)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred=clf.predict(X_test)\n",
    "    TT['score']=metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification with Cross Validation Portion 1 score 42.47 %\n",
      "Random Forest Classification with Cross Validation Portion 2 score 40.4 %\n",
      "Random Forest Classification with Cross Validation Portion 3 score 41.67 %\n",
      "Random Forest Classification with Cross Validation Portion 4 score 41.11 %\n"
     ]
    }
   ],
   "source": [
    "# Displaying scores for each Portion after training\n",
    "\n",
    "i = 1\n",
    "for each in Train_Test:\n",
    "    print(\"Random Forest Classification with Cross Validation Portion\",i, 'score',round(each['score']*100,2),'%')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using the best model (Dirty, Pure, Global) & the highest scored Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(predictionFunction, tagName = None):\n",
    "    \n",
    "    BEST_TRAINED_PARTITION = max( Train_Test, key=lambda i:i['score'])\n",
    "    #print(round(BEST_TRAINED_PARTITION['score']*100,2),'%')\n",
    "    \n",
    "    export = open(\"EPSI M2-ISD Valery & Charles predictions.txt\", \"wt\")\n",
    "    file   = open(\"test.txt\",\"r+\")\n",
    "    \n",
    "    for line in file:\n",
    "        export.write('('+ (predictionFunction(line[5:-1],BEST_TRAINED_PARTITION,False))+')'+line[5:-1]+'\\n')\n",
    "        \n",
    "    export.close()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Create Inline Prediction File\n",
    "export(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
