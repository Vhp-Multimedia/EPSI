{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from IPython.core.display import display, HTML\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import re\n",
    "import random\n",
    "import statistics\n",
    "text = open(\"train.txt\").readlines()\n",
    "textLen = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoupage du texte en cinq (k=5) sous parties\n",
    "def textToPortions(text,k):\n",
    "    portions=[]\n",
    "    increments = round(textLen/k)\n",
    "    for i in range(0,textLen,increments):\n",
    "        portions.append((i,i+increments-1))\n",
    "        \n",
    "    return portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portionsToSubtext(textArray, portions):\n",
    "    resText=[]\n",
    "    for i,j in portions:\n",
    "        for l in textArray[i:j+1]:\n",
    "            resText.append(l)\n",
    "            \n",
    "    return resText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle Text partitions\n",
    "#Input number of (train, test, validation partitions) = 5 \n",
    "\n",
    "def createShuffledSubText(originalText,train=None,test=None,validation=None):\n",
    "    \n",
    "    partitions = textToPortions(originalText,5)\n",
    "    result = { 'Train-Test': [], 'validation': partitions.pop()}\n",
    "    \n",
    "    for i in range(len(partitions)):\n",
    "        curr = {'train' : [partitions[i],partitions[(i+1)%4],partitions[(i+2)%4]], 'test' : [partitions[(i+3)%4]], 'language':{}, 'words':{}, 'score':0, 'failed':[]}\n",
    "        result['Train-Test'].append(curr)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nportions = textToPortions(text,100)\\nprocessedText = portionsToSubtext(text,portions)\\nprint(\"Test passed\" if len(processedText) == len(text) else \"Failed\")\\n'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "\"\"\"\n",
    "portions = textToPortions(text,100)\n",
    "processedText = portionsToSubtext(text,portions)\n",
    "print(\"Test passed\" if len(processedText) == len(text) else \"Failed\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = createShuffledSubText(text)\n",
    "Train_Test = Data ['Train-Test']\n",
    "Valid = Data['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(t):\n",
    "    # Input t as a paragraph, returns:\n",
    "    # list of (list of words,lenth of the list, and phrases count)\n",
    "    \n",
    "    t=t.lower()\n",
    "    tkns = []\n",
    "\n",
    "    phrases  = 0;\n",
    "    for s in t.split(\".\"):\n",
    "        phrases +=1\n",
    "        for w in s.split():\n",
    "            tkns.append(w)\n",
    "    return [tkns,len(tkns),phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCounter(row,language,TT):\n",
    "    \n",
    "    # Input the corpus as an array of inputs paragraphes, a target language, returns:\n",
    "    # list of (length of words in a dictionary, [distinct words count, sentences count], the dictionary of words)\n",
    "    \n",
    "    dict = {}\n",
    "    tk = tokenize(row)\n",
    "    \n",
    "    for w in tk[0]:\n",
    "        \n",
    "        # Processing the lang dictionary\n",
    "        dict[w]= ((dict[w][0])+1,'Pure') if w in dict else (1,'Pure')\n",
    "                                \n",
    "        #Processing the global dictionary\n",
    "        if w not in TT['words']: \n",
    "            TT['words'][w]={language:1}\n",
    "            \n",
    "        elif w in TT['words']:\n",
    "            \n",
    "            if language not in TT['words'][w]:\n",
    "                TT['words'][w][language]= 1\n",
    "            else:\n",
    "                TT['words'][w][language]+=1       \n",
    "                \n",
    "    return [len(dict.keys()),tk[1],tk[2],dict,TT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDict(d1, d2):\n",
    "    for w in d2:\n",
    "        if w not in d1:\n",
    "            d1[w]=d2[w]\n",
    "        else: \n",
    "            d1[w]=((d1[w][0]+d2[w][0]),d1[w][1])\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Count   \n",
    "for TT in Train_Test:\n",
    "    \n",
    "    train = portionsToSubtext(text,TT['train'])\n",
    "    \n",
    "    for i in range(len(train)):\n",
    "        lang = train[i][1:4]\n",
    "        wc = wordCounter(train[i][5:-1].lower(),lang,TT)\n",
    "        TT=wc[4]\n",
    "\n",
    "        if lang in TT['language']:\n",
    "\n",
    "            TT['language'][lang]['Size'     ]+=   1             #Taille\n",
    "            TT['language'][lang]['Distinct' ]+= wc[0]           #Unique Words\n",
    "            TT['language'][lang]['Words'    ]+= wc[1]           #All words \n",
    "            TT['language'][lang]['Phrases'  ]+= wc[2]           #Sentences\n",
    "            TT['language'][lang]['Avg'      ]+= 0               #Avg\n",
    "            TT['language'][lang]['meanWords']+= 0        #Avg Word Length\n",
    "            TT['language'][lang]['Dict'     ] = addDict(TT['language'][lang]['Dict'],wc[3])\n",
    "\n",
    "        else :\n",
    "\n",
    "            TT['language'][lang]= {'Size':1,'Distinct':wc[0],'Words':wc[1],'Phrases':wc[2], 'Avg':0 ,'meanWords':0,'Dict':wc[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train_Test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition [(0, 1979), (1980, 3959), (3960, 5939)] \n",
      "\n",
      "GER Total Words: 182772 Distinct Words: 84229 Total Phrases: 9274 AVG Word/Phrase: 19.7\n",
      "TUR Total Words: 179743 Distinct Words: 81300 Total Phrases: 10097 AVG Word/Phrase: 17.8\n",
      "CHI Total Words: 182279 Distinct Words: 81090 Total Phrases: 9881 AVG Word/Phrase: 18.4\n",
      "TEL Total Words: 186605 Distinct Words: 84139 Total Phrases: 9608 AVG Word/Phrase: 19.4\n",
      "ARA Total Words: 163522 Distinct Words: 75109 Total Phrases: 7185 AVG Word/Phrase: 22.8\n",
      "SPA Total Words: 184894 Distinct Words: 82879 Total Phrases: 7817 AVG Word/Phrase: 23.7\n",
      "HIN Total Words: 196553 Distinct Words: 91970 Total Phrases: 9929 AVG Word/Phrase: 19.8\n",
      "JPN Total Words: 165382 Distinct Words: 73065 Total Phrases: 10157 AVG Word/Phrase: 16.3\n",
      "KOR Total Words: 177133 Distinct Words: 78866 Total Phrases: 10660 AVG Word/Phrase: 16.6\n",
      "FRE Total Words: 186696 Distinct Words: 85711 Total Phrases: 9378 AVG Word/Phrase: 19.9\n",
      "ITA Total Words: 171516 Distinct Words: 80050 Total Phrases: 7424 AVG Word/Phrase: 23.1\n",
      "\n",
      "\n",
      "Partition [(1980, 3959), (3960, 5939), (5940, 7919)] \n",
      "\n",
      "TUR Total Words: 177272 Distinct Words: 79810 Total Phrases: 9825 AVG Word/Phrase: 18.0\n",
      "CHI Total Words: 188273 Distinct Words: 83951 Total Phrases: 10111 AVG Word/Phrase: 18.6\n",
      "KOR Total Words: 174938 Distinct Words: 77884 Total Phrases: 10556 AVG Word/Phrase: 16.6\n",
      "GER Total Words: 185807 Distinct Words: 85667 Total Phrases: 9526 AVG Word/Phrase: 19.5\n",
      "ARA Total Words: 165706 Distinct Words: 76023 Total Phrases: 7276 AVG Word/Phrase: 22.8\n",
      "HIN Total Words: 201744 Distinct Words: 94298 Total Phrases: 10294 AVG Word/Phrase: 19.6\n",
      "ITA Total Words: 172685 Distinct Words: 80867 Total Phrases: 7490 AVG Word/Phrase: 23.1\n",
      "FRE Total Words: 179698 Distinct Words: 82544 Total Phrases: 9064 AVG Word/Phrase: 19.8\n",
      "TEL Total Words: 189845 Distinct Words: 86290 Total Phrases: 9728 AVG Word/Phrase: 19.5\n",
      "SPA Total Words: 178427 Distinct Words: 79491 Total Phrases: 7477 AVG Word/Phrase: 23.9\n",
      "JPN Total Words: 161828 Distinct Words: 71622 Total Phrases: 9894 AVG Word/Phrase: 16.4\n",
      "\n",
      "\n",
      "Partition [(3960, 5939), (5940, 7919), (0, 1979)] \n",
      "\n",
      "TUR Total Words: 180132 Distinct Words: 81487 Total Phrases: 9889 AVG Word/Phrase: 18.2\n",
      "JPN Total Words: 159885 Distinct Words: 70955 Total Phrases: 9834 AVG Word/Phrase: 16.3\n",
      "KOR Total Words: 176320 Distinct Words: 78334 Total Phrases: 10637 AVG Word/Phrase: 16.6\n",
      "ARA Total Words: 165999 Distinct Words: 75703 Total Phrases: 7304 AVG Word/Phrase: 22.7\n",
      "CHI Total Words: 185593 Distinct Words: 82258 Total Phrases: 9886 AVG Word/Phrase: 18.8\n",
      "FRE Total Words: 186422 Distinct Words: 85993 Total Phrases: 9333 AVG Word/Phrase: 20.0\n",
      "TEL Total Words: 183540 Distinct Words: 83322 Total Phrases: 9396 AVG Word/Phrase: 19.5\n",
      "HIN Total Words: 204017 Distinct Words: 95258 Total Phrases: 10252 AVG Word/Phrase: 19.9\n",
      "GER Total Words: 187857 Distinct Words: 86239 Total Phrases: 9661 AVG Word/Phrase: 19.4\n",
      "SPA Total Words: 187102 Distinct Words: 83171 Total Phrases: 7931 AVG Word/Phrase: 23.6\n",
      "ITA Total Words: 165841 Distinct Words: 77691 Total Phrases: 7283 AVG Word/Phrase: 22.8\n",
      "\n",
      "\n",
      "Partition [(5940, 7919), (0, 1979), (1980, 3959)] \n",
      "\n",
      "CHI Total Words: 193342 Distinct Words: 86457 Total Phrases: 10466 AVG Word/Phrase: 18.5\n",
      "ITA Total Words: 174864 Distinct Words: 81888 Total Phrases: 7608 AVG Word/Phrase: 23.0\n",
      "SPA Total Words: 185798 Distinct Words: 83118 Total Phrases: 7825 AVG Word/Phrase: 23.7\n",
      "KOR Total Words: 172130 Distinct Words: 77093 Total Phrases: 10255 AVG Word/Phrase: 16.8\n",
      "JPN Total Words: 161718 Distinct Words: 71035 Total Phrases: 9928 AVG Word/Phrase: 16.3\n",
      "TEL Total Words: 180959 Distinct Words: 82213 Total Phrases: 9401 AVG Word/Phrase: 19.2\n",
      "GER Total Words: 185590 Distinct Words: 85562 Total Phrases: 9447 AVG Word/Phrase: 19.6\n",
      "ARA Total Words: 160981 Distinct Words: 73603 Total Phrases: 7011 AVG Word/Phrase: 23.0\n",
      "FRE Total Words: 185298 Distinct Words: 85154 Total Phrases: 9296 AVG Word/Phrase: 19.9\n",
      "TUR Total Words: 170487 Distinct Words: 77200 Total Phrases: 9555 AVG Word/Phrase: 17.8\n",
      "HIN Total Words: 197402 Distinct Words: 92367 Total Phrases: 10043 AVG Word/Phrase: 19.7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for TT in Train_Test:\n",
    "    #Taille Moyenne des phrase en mots\n",
    "    print(\"Partition\", TT['train'],'\\n')\n",
    "    for l in TT['language']:\n",
    "        TT['language'][l]['Avg']=round(TT['language'][l]['Words']/TT['language'][l]['Phrases'],1)\n",
    "\n",
    "        print(l,\"Total Words:\",TT['language'][l]['Words'],\"Distinct Words:\",TT['language'][l]['Distinct'],\"Total Phrases:\",TT['language'][l]['Phrases'],\"AVG Word/Phrase:\",TT['language'][l]['Avg'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(a,b):\n",
    "    \n",
    "    for w in a:\n",
    "        \"\"\"\n",
    "        if type(a[w])=='List' and a[w][1]=='Dirty':\n",
    "            continue;\n",
    "        if type(a[w])=='int' and w in b:\n",
    "           \n",
    "            a[w]=[a[w],'Dirty']         \n",
    "        else :\n",
    "            a[w]=[a[w],'Pure']\n",
    "        \"\"\"\n",
    "        if a[w][1]=='Dirty':\n",
    "            continue;\n",
    "        elif w in b: \n",
    "            a[w]=(a[w][0],'Dirty')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for TT in Train_Test:\n",
    "    \n",
    "    langs = list(TT['language'].keys())\n",
    "    size  = len(langs)\n",
    "\n",
    "    for x in range(size):\n",
    "        for y in range(size):\n",
    "            if x != y:\n",
    "                TT['language'][langs[x]]['D'] = extract(TT['language'][langs[x]]['Dict'],TT['language'][langs[y]]['Dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GER': 8041,\n",
       " 'TUR': 8661,\n",
       " 'CHI': 9195,\n",
       " 'TEL': 10541,\n",
       " 'ARA': 7324,\n",
       " 'SPA': 9064,\n",
       " 'HIN': 9937,\n",
       " 'JPN': 6518,\n",
       " 'KOR': 6700,\n",
       " 'FRE': 7858,\n",
       " 'ITA': 8352}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Test[0]['words']['the']\n",
    "#Train_Test[0]['language']['GER']['Dict']['ithe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for TT in Train_Test:\n",
    "    #Testing acuracy of both dictionaries \n",
    "    for word in TT['words'].keys():\n",
    "        for lang in TT['words'][word].keys():\n",
    "            if word not in TT['language'][lang]['Dict']:\n",
    "                print(TT['train'],word)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter(l1, l2):\n",
    "    dict = {}\n",
    "    for e in l2:\n",
    "        if e in l1:\n",
    "            dict[e]=0\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magicBox(t):\n",
    "    \n",
    "    words=tokenize(t)[0]\n",
    "    langs = list(language.keys())\n",
    "            \n",
    "    LT    = len(words)\n",
    "    stats = {}\n",
    "    \n",
    "    for lang in langs: \n",
    "        \n",
    "        D = language[lang]['Dict']\n",
    "        LD = len(D)\n",
    "        score = 0\n",
    "\n",
    "        for w in words:\n",
    "            score += 1 if w in D and D[w][1]=='Pure'else 0\n",
    "            \n",
    "        stats[lang]=(score/LD)\n",
    "    print(stats)\n",
    "        \n",
    "    return max(stats.keys(), key=(lambda k: stats[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box(t,TT,debug):\n",
    "    \n",
    "    tokens =tokenize(t)[0]\n",
    "    \n",
    "    hasSetPure   = False\n",
    "    hasSetDirty  = False\n",
    "    hasSetGlobal = False\n",
    "    \n",
    "    exequos = []\n",
    "    \n",
    "    langStats = {\n",
    "        \n",
    "        'FRE':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } },\n",
    "        'ARA':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } },\n",
    "        'CHI':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } },\n",
    "        'TUR':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } },\n",
    "        'TEL':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } },\n",
    "        'GER':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } },\n",
    "        'JPN':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } },\n",
    "        'HIN':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } },\n",
    "        'SPA':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } },\n",
    "        'KOR':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } },\n",
    "        'ITA':{ 'pure':0, 'unknown':0, 'global':0,'dirty':{'GER':0, 'TEL':0, 'SPA':0, 'KOR':0, 'JPN':0, 'ARA':0, 'FRE':0, 'HIN':0, 'TUR':0, 'CHI':0, 'ITA':0 } }\n",
    "    }\n",
    "     \n",
    "    for each in list(langStats.keys()):\n",
    "        \n",
    "        D = TT['language'][each]['Dict']\n",
    "        for w in tokens:\n",
    "            \n",
    "            if w in D: \n",
    "                \n",
    "                if D[w][1]=='Pure':\n",
    "                    langStats[each]['pure']+=round(1/len(D),7)\n",
    "                    hasSetPure=True;\n",
    "                    \n",
    "                elif D[w][1]=='Dirty':\n",
    "                    for e in TT['words'][w].keys():\n",
    "                        langStats[each]['dirty'][e]+=TT['words'][w][e]\n",
    "                        hasSetDirty=True;\n",
    "                        \n",
    "            elif w in TT['words']:\n",
    "                langStats[each]['global']+=round(1/len(TT['words']), 7)\n",
    "                hasSetGloabl=True;\n",
    "            else :\n",
    "                langStats[each]['unknown']+=1\n",
    "                \n",
    "        if debug :\n",
    "            print(each,langStats[each])\n",
    "        \n",
    "    #Traitement des globaux_excluded       \n",
    "    if not hasSetPure:\n",
    "        return min(langStats.keys(), key=(lambda k: langStats[k]['global']))\n",
    "    else :\n",
    "        return max(langStats.keys(), key=(lambda k: langStats[k]['pure']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score (TT): \n",
    "    \n",
    "    Test = portionsToSubtext(text,TT['test'])\n",
    "    acc = 0\n",
    "    failed = []\n",
    "    L = len(Test)\n",
    "    for i in range(L):\n",
    "\n",
    "        T=Test[i]\n",
    "        trueLang = T[1:4]\n",
    "\n",
    "        predictedLang = box(T[5:-1],TT,False)\n",
    "        if predictedLang != trueLang:\n",
    "            #print(i,trueLang,predictedLang)\n",
    "            failed.append(i)\n",
    "\n",
    "        if predictedLang == trueLang:\n",
    "            acc+=1/L\n",
    "    return acc,failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for TT in Train_Test:\n",
    "    TT['score'],TT['failed']=score(TT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portion 1 score 25.56 %\n",
      "Portion 2 score 26.46 %\n",
      "Portion 3 score 24.75 %\n",
      "Portion 4 score 25.66 %\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for each in Train_Test:\n",
    "    print(\"Portion\",i, 'score',round(each['score']*100,2),'%')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portion 1 score 30.15 %\n",
      "Portion 2 score 30.45 %\n",
      "Portion 3 score 29.04 %\n",
      "Portion 4 score 29.8 %\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for each in Train_Test:\n",
    "    print(\"Portion\",i, 'score',round(each['score']*100,2),'%')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i in Train_Test[0]['failed']:\\n    Text = portionsToSubtext(text,Train_Test[0]['test'])\\n    box(Text[i],Train_Test[0],True)\\n    print('--')\\n\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analysing failed\n",
    "\"\"\"for i in Train_Test[0]['failed']:\n",
    "    Text = portionsToSubtext(text,Train_Test[0]['test'])\n",
    "    box(Text[i],Train_Test[0],True)\n",
    "    print('--')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeature(T,TT):\n",
    "    tk = tokenize(T[5:-1])\n",
    "    return [len(TT['language'][T[1:4]]['Dict']),len(t),tk[1], tk[2],(tk[1]/tk[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(TT['language'][T[1:4]]['Dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classification with Cross Validation\n",
    "for TT in Train_Test:\n",
    "    \n",
    "    Train = portionsToSubtext(text,TT['train'])\n",
    "    Test  = portionsToSubtext(text,TT['test' ])\n",
    "    \n",
    "    X_train =[] \n",
    "    y_train =[]\n",
    "\n",
    "    for t in Train:\n",
    "        y_train.append(t[1:4])\n",
    "        X_train.append(createFeature(t,TT))\n",
    "        \n",
    "    X_test =[] \n",
    "    y_test =[]\n",
    "\n",
    "    for t in Test:\n",
    "        y_test.append(t[1:4])\n",
    "        X_test.append(createFeature(t,TT))\n",
    "\n",
    "    clf=RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred=clf.predict(X_test)\n",
    "    TT['score']=metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification with Cross Validation Portion 1 score 14.14 %\n",
      "Random Forest Classification with Cross Validation Portion 2 score 13.79 %\n",
      "Random Forest Classification with Cross Validation Portion 3 score 13.08 %\n",
      "Random Forest Classification with Cross Validation Portion 4 score 14.19 %\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for each in Train_Test:\n",
    "    print(\"Random Forest Classification with Cross Validation Portion\",i, 'score',round(each['score']*100,2),'%')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "langues = ['FRE','ARA','CHI','TUR','TEL','GER','JPN','HIN','SPA','KOR','ITA']\n",
    "def langue(texte):\n",
    "    return texte[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boite_vide(mot):\n",
    "    return (mot,\n",
    "           {'ALL' : 11,\n",
    "            'GER' : 1,\n",
    "            'TUR' : 1,\n",
    "            'CHI' : 1,\n",
    "            'TEL' : 1,\n",
    "            'ARA' : 1,\n",
    "            'SPA' : 1,\n",
    "            'HIN' : 1,\n",
    "            'JPN' : 1,\n",
    "            'KOR' : 1,\n",
    "            'FRE' : 1,\n",
    "            'ITA' : 1},\n",
    "           {'ALL' : 11,\n",
    "            'GER' : 1,\n",
    "            'TUR' : 1,\n",
    "            'CHI' : 1,\n",
    "            'TEL' : 1,\n",
    "            'ARA' : 1,\n",
    "            'SPA' : 1,\n",
    "            'HIN' : 1,\n",
    "            'JPN' : 1,\n",
    "            'KOR' : 1,\n",
    "            'FRE' : 1,\n",
    "            'ITA' : 1},\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entraine(boite,texte,langue):\n",
    "    mot,present,absent = boite\n",
    "    if mot in texte :\n",
    "        present['ALL']  = present['ALL']  + 1\n",
    "        present[langue] = present[langue] + 1\n",
    "    else :\n",
    "        absent['ALL']   = absent['ALL']  + 1\n",
    "        absent[langue]  = absent[langue] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalue(boite,texte):\n",
    "    mot,present,absent = boite\n",
    "    if mot in texte :\n",
    "        return (True,present,present['ALL'])\n",
    "    else:\n",
    "        return (False,absent,present['ALL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cree_boites(mots):\n",
    "    boites = []\n",
    "    for mot in mots:\n",
    "        boites.append(boite_vide(mot))\n",
    "    return boites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entraine_boites(boites,textes):\n",
    "    for texte in textes :\n",
    "        texte = tokenize(text[5:-1])[0]\n",
    "        l = langue(texte)\n",
    "        for boite in boite :\n",
    "            entraine(boite,texte,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cree_boites_entrainees(mots,textes):\n",
    "    #creation des boites vides\n",
    "    boites = {}\n",
    "    for mot in list(mots):\n",
    "        boites[mot] = boite_vide(mot)\n",
    "    \n",
    "    #ajout des mots «présents»\n",
    "    nb = {'GER' : 0,'TUR' : 0,'CHI' : 0,'TEL' : 0,'ARA' : 0,'SPA' : 0,'HIN' : 0,'JPN' : 0,'KOR' : 0,'FRE' : 0,'ITA' : 0}\n",
    "    for texte in textes :\n",
    "        langue = texte[1:4]\n",
    "        nb[langue] += 1\n",
    "        mots = set()\n",
    "        mots.update(tokenize(texte[5:-1])[0])\n",
    "        for mot in mots :\n",
    "            entraine(boites[mot],[mot],langue)\n",
    "            \n",
    "    #evaluation des mots «absents»\n",
    "    for boite in boites.values() :\n",
    "        mot,present,absent = boite\n",
    "        absent['ALL'] = len(textes) - present['ALL']\n",
    "        for langue in langues :\n",
    "            absent[langue] = nb[langue] - present[langue]\n",
    "    \n",
    "    return boites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(acc,resultat,simple):\n",
    "    s,v,m = resultat\n",
    "    if simple or s:\n",
    "        for i in langues :\n",
    "            acc[i] += v[i]*m\n",
    "def evaluer(boites,texte,simple = True,f = f):\n",
    "    acc = {'GER' : 0,'TUR' : 0,'CHI' : 0,'TEL' : 0,'ARA' : 0,'SPA' : 0,'HIN' : 0,'JPN' : 0,'KOR' : 0,'FRE' : 0,'ITA' : 0}\n",
    "    for boite in boites :\n",
    "        f(acc,evalue(boite,texte),simple)\n",
    "    m = 0\n",
    "    langue = None\n",
    "    for l in list(acc.keys()):\n",
    "        if acc[l] > m :\n",
    "            m = acc[l]\n",
    "            langue = l\n",
    "    return langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(acc,resultat,simple):\n",
    "    s,v,m = resultat\n",
    "    if simple or s:\n",
    "        for i in langues :\n",
    "            acc[i] += (v[i])\n",
    "def evaluer(boites,texte,simple = True,f = f):\n",
    "    init = 1\n",
    "    acc = {'GER' : init,'TUR' : init,'CHI' : init,'TEL' : init,'ARA' : init,'SPA' : init,'HIN' : init,'JPN' : init,'KOR' : init,'FRE' : init,'ITA' : init}\n",
    "    for boite in boites :\n",
    "        f(acc,evalue(boite,texte),simple)\n",
    "    m = 0\n",
    "    langue = None\n",
    "    for l in list(acc.keys()):\n",
    "        if acc[l] > m :\n",
    "            m = acc[l]\n",
    "            langue = l\n",
    "    return langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "boites = cree_boites_entrainees(list(words.keys()),text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JPN'"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluer(list(boites.values()),tokenize(text[0][5:-1])[0],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(boites,texte = text) :\n",
    "    s = 0\n",
    "    for t in texte :\n",
    "        if evaluer(boites,tokenize(t[5:-1])[0]) == langue(t):\n",
    "            s += 1\n",
    "    return s/len(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(list(boites.values()),text[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y|Avg Word/Phrases|Total_Words|Distinct_Words|Total_Phrases|AVG Word/Phrase|pure_count|dirty count|globaux_excluded|'dirty': {'GER': 295926, 'TEL': 266660, 'SPA': 319469, 'KOR': 264377, 'JPN': 251467, 'ARA': 257221, 'FRE': 317691, 'HIN': 289154, 'TUR': 280723, 'CHI': 301288, 'ITA': 284233}|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(predict, corpus_path):\n",
    "    \n",
    "    export = open(\"Charles & Valery - predicted_\"+corpus_path, \"wt\")\n",
    "    file   = open(corpus_file_path,\"r+\")\n",
    "    \n",
    "    for line in file:\n",
    "        export.write('('+ (predict(line[5:-1]))+')'+line[5:-1])\n",
    "        \n",
    "    export.close()\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
